{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510fa83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic Solution:\n",
      "Coefficients: [1.23636364 1.16969697]\n",
      "Sum of Squared Errors: 5.624242424242426\n",
      "R^2 value: 0.952538038613988\n",
      "\n",
      "Full-batch Gradient Descent:\n",
      "Coefficients: (1.170263693076768, 1.2328099487610318)\n",
      "Sum of Squared Errors: 5.624278989977716\n",
      "R^2 value: 0.9525377300423822\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "Coefficients: (1.2986755729435908, 0.8967040680508923)\n",
      "Sum of Squared Errors: 7.576246971879953\n",
      "R^2 value: 0.9360654263976376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "X = np.vstack([np.ones(len(x)), x]).T\n",
    "beta_hat_normal_eq = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "y_hat_normal_eq = X @ beta_hat_normal_eq\n",
    "SSE_normal_eq = np.sum((y - y_hat_normal_eq)**2)\n",
    "R_squared_normal_eq = 1 - SSE_normal_eq / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"Analytic Solution:\")\n",
    "print(f\"Coefficients: {beta_hat_normal_eq}\")\n",
    "print(f\"Sum of Squared Errors: {SSE_normal_eq}\")\n",
    "print(f\"R^2 value: {R_squared_normal_eq}\") \n",
    "def full_batch_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
    "    m, b = 0.0, 0.0\n",
    "    N = len(y)\n",
    "    for _ in range(epochs):\n",
    "        y_pred = m*x + b\n",
    "        dm = (-2/N) * np.sum(x * (y - y_pred))\n",
    "        db = (-2/N) * np.sum(y - y_pred)\n",
    "        m -= lr * dm\n",
    "        b -= lr * db\n",
    "    return m, b\n",
    "m_gd, b_gd = full_batch_gradient_descent(x, y)\n",
    "y_pred_gd = m_gd*x + b_gd\n",
    "sse_gd = np.sum((y - y_pred_gd)**2)\n",
    "r_squared_gd = 1 - sse_gd / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(f\"Coefficients: {m_gd, b_gd}\")\n",
    "print(f\"Sum of Squared Errors: {sse_gd}\")\n",
    "print(f\"R^2 value: {r_squared_gd}\")\n",
    "def stochastic_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
    "    m, b = 0.0, 0.0\n",
    "    N = len(y)\n",
    "    for _ in range(epochs):\n",
    "        for i in range(N):\n",
    "            y_pred = m*x[i] + b\n",
    "            dm = -2 * x[i] * (y[i] - y_pred)\n",
    "            db = -2 * (y[i] - y_pred)\n",
    "            m -= lr * dm\n",
    "            b -= lr * db\n",
    "    return m, b\n",
    "m_sgd, b_sgd = stochastic_gradient_descent(x, y)\n",
    "y_pred_sgd = m_sgd*x + b_sgd\n",
    "sse_sgd = np.sum((y - y_pred_sgd)**2)\n",
    "r_squared_sgd = 1 - sse_sgd / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(f\"Coefficients: {m_sgd, b_sgd}\")\n",
    "print(f\"Sum of Squared Errors: {sse_sgd}\")\n",
    "print(f\"R^2 value: {r_squared_sgd}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "        b  lstat  medv  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "4  396.90   5.33  36.2  \n",
      "The attribute that best follows the linear relationship with the output price is: lstat\n",
      "Analytic solution coefficients: [ 2.25328063e+01 -9.28146064e-01  1.08156863e+00  1.40899997e-01\n",
      "  6.81739725e-01 -2.05671827e+00  2.67423017e+00  1.94660717e-02\n",
      " -3.10404426e+00  2.66221764e+00 -2.07678168e+00 -2.06060666e+00\n",
      "  8.49268418e-01 -3.74362713e+00]\n",
      "Gradient descent (Full-batch) coefficients (scaled features): [ 2.25328063e+01 -9.28146063e-01  1.08156863e+00  1.40899992e-01\n",
      "  6.81739726e-01 -2.05671827e+00  2.67423017e+00  1.94660708e-02\n",
      " -3.10404426e+00  2.66221763e+00 -2.07678167e+00 -2.06060666e+00\n",
      "  8.49268418e-01 -3.74362713e+00]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "boston_data = pd.read_csv(\"BostonHousing.csv\")\n",
    "print(boston_data.head())\n",
    "X = boston_data.drop(columns=[\"medv\"]) \n",
    "y = boston_data[\"medv\"] \n",
    "correlation_coeffs = np.abs(boston_data.corr()[\"medv\"]).drop(\"medv\")\n",
    "best_feature_name = correlation_coeffs.idxmax()\n",
    "\n",
    "print(f\"The attribute that best follows the linear relationship with the output price is: {best_feature_name}\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_with_bias = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "analytic_solution = np.linalg.inv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
    "\n",
    "print(\"Analytic solution coefficients:\", analytic_solution)\n",
    "class LinearRegressionGradientDescent:\n",
    "    def __init__(self, lr=0.0001, max_iter=10000, tolerance=1e-6):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.weights = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            gradients = -2 * X_with_bias.T.dot(y - X_with_bias.dot(self.weights))\n",
    "            if np.all(np.abs(gradients) < self.tolerance):\n",
    "                break\n",
    "            self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_with_bias.dot(self.weights)\n",
    "lr_full_batch_scaled = LinearRegressionGradientDescent(lr=0.0001, max_iter=10000)\n",
    "lr_full_batch_scaled.fit(X_scaled, y)\n",
    "\n",
    "print(\"Gradient descent (Full-batch) coefficients (scaled features):\", lr_full_batch_scaled.weights)\n",
    "class LinearRegressionStochasticGradientDescent:\n",
    "    def __init__(self, lr=0.0001, max_iter=1000, tolerance=1e-6, batch_size=1):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.batch_size = batch_size\n",
    "    def fit(self, X, y):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.weights = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            indices = np.random.permutation(X_with_bias.shape[0])\n",
    "            for start_idx in range(0, X_with_bias.shape[0], self.batch_size):\n",
    "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
    "                X_batch = X_with_bias[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                gradients = -2 * X_batch.T.dot(y_batch - X_batch.dot(self.weights))\n",
    "                if np.all(np.abs(gradients) < self.tolerance):\n",
    "                    break\n",
    "                self.weights -= self.lr * gradients\n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_with_bias.dot(self.weights)\n",
    "lr_stochastic_scaled = LinearRegressionStochasticGradientDescent(lr=0.0001, max_iter=10000, batch_size=1)\n",
    "lr_stochastic_scaled.fit(X_scaled, y)\n",
    "print(\"Gradient descent (Stochastic) coefficients (scaled features):\", lr_stochastic_scaled.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b8f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
